{
    "messages": [
        {
            "username": "brian_spiering", 
            "display_as_bot": false, 
            "text": "<@U1XU9SHRS|brian_spiering> uploaded a file: <https://gstudents.slack.com/files/brian_spiering/F5DQQ8997/screen_shot_2017-05-16_at_4.54.15_pm.png|Screen Shot 2017-05-16 at 4.54.15 PM.png>", 
            "upload": true, 
            "ts": "1494978912.799801", 
            "subtype": "file_share", 
            "user": "U1XU9SHRS", 
            "file": {
                "thumb_480_w": 480, 
                "groups": [], 
                "filetype": "png", 
                "thumb_800_h": 766, 
                "thumb_480": "https://files.slack.com/files-tmb/T1T555TL0-F5DQQ8997-5982963001/screen_shot_2017-05-16_at_4.54.15_pm_480.png", 
                "display_as_bot": false, 
                "thumb_800_w": 800, 
                "thumb_64": "https://files.slack.com/files-tmb/T1T555TL0-F5DQQ8997-5982963001/screen_shot_2017-05-16_at_4.54.15_pm_64.png", 
                "size": 390106, 
                "original_h": 773, 
                "thumb_360_w": 360, 
                "title": "Screen Shot 2017-05-16 at 4.54.15 PM.png", 
                "url_private": "https://files.slack.com/files-pri/T1T555TL0-F5DQQ8997/screen_shot_2017-05-16_at_4.54.15_pm.png", 
                "thumb_720_h": 690, 
                "thumb_360": "https://files.slack.com/files-tmb/T1T555TL0-F5DQQ8997-5982963001/screen_shot_2017-05-16_at_4.54.15_pm_360.png", 
                "id": "F5DQQ8997", 
                "ims": [], 
                "thumb_720_w": 720, 
                "thumb_80": "https://files.slack.com/files-tmb/T1T555TL0-F5DQQ8997-5982963001/screen_shot_2017-05-16_at_4.54.15_pm_80.png", 
                "thumb_360_h": 345, 
                "thumb_480_h": 460, 
                "external_type": "", 
                "username": "", 
                "thumb_800": "https://files.slack.com/files-tmb/T1T555TL0-F5DQQ8997-5982963001/screen_shot_2017-05-16_at_4.54.15_pm_800.png", 
                "timestamp": 1494978907, 
                "public_url_shared": false, 
                "editable": false, 
                "thumb_160": "https://files.slack.com/files-tmb/T1T555TL0-F5DQQ8997-5982963001/screen_shot_2017-05-16_at_4.54.15_pm_160.png", 
                "url_private_download": "https://files.slack.com/files-pri/T1T555TL0-F5DQQ8997/download/screen_shot_2017-05-16_at_4.54.15_pm.png", 
                "user": "U1XU9SHRS", 
                "image_exif_rotation": 1, 
                "is_public": true, 
                "pretty_type": "PNG", 
                "name": "Screen Shot 2017-05-16 at 4.54.15 PM.png", 
                "mimetype": "image/png", 
                "permalink_public": "https://slack-files.com/T1T555TL0-F5DQQ8997-a3bdb15547", 
                "permalink": "https://gstudents.slack.com/files/brian_spiering/F5DQQ8997/screen_shot_2017-05-16_at_4.54.15_pm.png", 
                "is_external": false, 
                "created": 1494978907, 
                "original_w": 807, 
                "comments_count": 0, 
                "mode": "hosted", 
                "thumb_720": "https://files.slack.com/files-tmb/T1T555TL0-F5DQQ8997-5982963001/screen_shot_2017-05-16_at_4.54.15_pm_720.png", 
                "channels": [
                    "C4MMDP3AP"
                ]
            }, 
            "type": "message", 
            "bot_id": null
        }, 
        {
            "text": "i can\u2019t decide if this is the stuff of dreams or nightmares:", 
            "type": "message", 
            "user": "U1XU9SHRS", 
            "ts": "1494978895.795648"
        }, 
        {
            "text": "<https://simons.berkeley.edu/talks/daniel-roy-10-06-2016>", 
            "type": "message", 
            "user": "U1XU9SHRS", 
            "ts": "1494456942.046188", 
            "attachments": [
                {
                    "title": "A Personal Viewpoint on Probabilistic Programming | Simons Institute for the Theory of Computing", 
                    "text": "Probabilistic programming is, in the abstract, the study of algorithmic processes that represent and transform uncertainty. In practice, there are many probabilistic programming systems that, to varying degrees of generality and efficiency, allow users to characterize states of uncertainty via probability models and update those models in light of data, either exactly or approximately. I will give a survey of the field and characterize some challenges ahead.", 
                    "title_link": "https://simons.berkeley.edu/talks/daniel-roy-10-06-2016", 
                    "service_name": "simons.berkeley.edu", 
                    "id": 1, 
                    "fallback": "A Personal Viewpoint on Probabilistic Programming | Simons Institute for the Theory of Computing", 
                    "service_icon": "https://simons.berkeley.edu/sites/all/themes/simons/favicon.ico", 
                    "from_url": "https://simons.berkeley.edu/talks/daniel-roy-10-06-2016"
                }
            ]
        }, 
        {
            "text": "i\u2019m going deep", 
            "type": "message", 
            "user": "U1XU9SHRS", 
            "ts": "1494456932.043549"
        }, 
        {
            "reactions": [
                {
                    "count": 2, 
                    "name": "brain", 
                    "users": [
                        "U233TELHZ", 
                        "U21RUE8QZ"
                    ]
                }
            ], 
            "attachments": [
                {
                    "author_link": "http://en.wikipedia.org/", 
                    "text": "A probabilistic programming language (PPL) is a programming language designed to describe probabilistic models and then perform inference in those models. PPLs are closely related to graphical models and Bayesian networks, but are more expressive and flexible. Probabilistic programming represents an attempt to \"[unify] general purpose programming with probabilistic modeling.\"\nProbabilistic reasoning is a foundational technology of machine learning. It is used by companies such as Google, <http://Amazon.com|Amazon.com> and Microsoft. Probabilistic reasoning has been used for predicting stock prices, recommending movies, diagnosing computers, detecting cyber intrusions and image detection.\nPPLs often extend from a basic language. The choice of underlying basic language depends on the similarity of the model to the basic language's ontology, as well as commercial considerations and personal preference. For instance, Dimple and Chimple are based on Java, <http://Infer.NET|Infer.NET> is based on .NET framework, while PRISM extends from Prolog. However, some PPLs such as WinBUGS and Stan offer a self-contained language, with no obvious origin in another language.\nSeveral PPLs are in active development, including some in beta test.", 
                    "title": "Probabilistic programming language", 
                    "author_name": "Wikipedia", 
                    "title_link": "https://en.wikipedia.org/wiki/Probabilistic_programming_language", 
                    "fallback": "wikipedia: Probabilistic programming language", 
                    "service_icon": "https://fst.slack-edge.com/bfaba/img/unfurl_icons/wikipedia.png", 
                    "id": 1
                }, 
                {
                    "title": "Probabilistic Programming and Digital Humanities", 
                    "service_name": "Talking Machines", 
                    "title_link": "http://www.thetalkingmachines.com/blog/2015/11/5/probabilistic-programming-and-digital-humanities", 
                    "text": "In episode 23 we talk with David Mimno of Cornell University about his work in the digital humanities (and explore what machine learning can tell us about lady zombie ghosts and huge bodies of literature) Ryan introduces us to probabilistic programming\u00a0and we take a listener question about k", 
                    "id": 2, 
                    "fallback": "Talking Machines: Probabilistic Programming and Digital Humanities", 
                    "service_icon": "http://www.thetalkingmachines.com/favicon.ico", 
                    "from_url": "http://www.thetalkingmachines.com/blog/2015/11/5/probabilistic-programming-and-digital-humanities"
                }, 
                {
                    "title": "ANGLICAN and Probabilistic Programming", 
                    "service_name": "Talking Machines", 
                    "title_link": "http://www.thetalkingmachines.com/blog/2016/10/1/anglican-and-probabilistic-programming", 
                    "text": "In episode seventeen of season two we get an introduction to Min Hashing, talk with Frank Wood the creator of ANGLICAN, about probabilistic programming and his new company, INVREA, and take a listener question about how to choose an architecture when using a neural network.", 
                    "id": 3, 
                    "fallback": "Talking Machines: ANGLICAN and Probabilistic Programming", 
                    "service_icon": "http://www.thetalkingmachines.com/favicon.ico", 
                    "from_url": "http://www.thetalkingmachines.com/blog/2016/10/1/anglican-and-probabilistic-programming"
                }, 
                {
                    "thumb_height": 432, 
                    "service_icon": "https://public.slidesharecdn.com/b/favicon.ico?d8e2a4ed15", 
                    "thumb_width": 768, 
                    "title": "Probabilistic Programming: Why, What, How, When?", 
                    "text": "Probabilistic programming is a new approach to machine learning and data science that is currently the focus of intense academic research, including an ongoing\u2026", 
                    "title_link": "https://www.slideshare.net/salesforceeng/probabalistic-programming-why-what-how-when", 
                    "service_name": "slideshare.net", 
                    "id": 4, 
                    "fallback": "Probabilistic Programming: Why, What, How, When?", 
                    "thumb_url": "https://cdn.slidesharecdn.com/ss_thumbnails/strata2014probabilisticprogramming-140627155634-phpapp02-thumbnail-4.jpg?cb=1403884734", 
                    "from_url": "https://www.slideshare.net/salesforceeng/probabalistic-programming-why-what-how-when"
                }
            ], 
            "text": "<@U21RUE8QZ> <@U233TELHZ> \n<https://en.wikipedia.org/wiki/Probabilistic_programming_language>\n<http://www.thetalkingmachines.com/blog/2015/11/5/probabilistic-programming-and-digital-humanities>\n<http://www.thetalkingmachines.com/blog/2016/10/1/anglican-and-probabilistic-programming>\n<https://www.slideshare.net/salesforceeng/probabalistic-programming-why-what-how-when>", 
            "ts": "1494437023.629527", 
            "user": "U1XU9SHRS", 
            "type": "message"
        }, 
        {
            "text": "lol, but still cool that there are jobs like this", 
            "type": "message", 
            "user": "U21RUE8QZ", 
            "ts": "1493843151.122985"
        }, 
        {
            "edited": {
                "user": "U1XU9SHRS", 
                "ts": "1493829031.000000"
            }, 
            "attachments": [
                {
                    "thumb_height": 600, 
                    "service_icon": "https://d1a3f4spazzrp4.cloudfront.net/uber-com/1.1.14/d1a3f4spazzrp4.cloudfront.net/images/apple-touch-icon-db8765b06b.png", 
                    "thumb_width": 600, 
                    "title": "Find Your Uber Career", 
                    "text": "Take your ideas to the next level and make something that people use everyday with a career at Uber.", 
                    "title_link": "https://careers-uber.icims.com/jobs/30828/job?iis=job-V002A-eng&iisn=Indeed&rx_campaign=indeed0&rx_group=101535&rx_job=30828&rx_source=Indeed&mobile=false&width=1208&height=500&bga=true&needsRedirect=false&jan1offset=-480&jun1offset=-420", 
                    "service_name": "careers-uber.icims.com", 
                    "id": 1, 
                    "fallback": "Find Your Uber Career", 
                    "thumb_url": "https://d1a3f4spazzrp4.cloudfront.net/uber-com/1.1.14/d1a3f4spazzrp4.cloudfront.net/images/seo-image-30cc8ae4a5.jpg", 
                    "from_url": "https://careers-uber.icims.com/jobs/30828/job?iis=job-V002A-eng&iisn=Indeed&rx_campaign=indeed0&rx_group=101535&rx_job=30828&rx_source=Indeed&mobile=false&width=1208&height=500&bga=true&needsRedirect=false&jan1offset=-480&jun1offset=-420"
                }
            ], 
            "text": "(i know it is is Uber)\n\nBayesian Optimization Research Scientist\n<https://careers-uber.icims.com/jobs/30828/job?iis=job-V002A-eng&amp;iisn=Indeed&amp;rx_campaign=indeed0&amp;rx_group=101535&amp;rx_job=30828&amp;rx_source=Indeed&amp;mobile=false&amp;width=1208&amp;height=500&amp;bga=true&amp;needsRedirect=false&amp;jan1offset=-480&amp;jun1offset=-420>", 
            "ts": "1493829011.696703", 
            "user": "U1XU9SHRS", 
            "type": "message"
        }, 
        {
            "text": "<https://www.youtube.com/watch?v=DGJTEBt0d-s>", 
            "type": "message", 
            "user": "U1XU9SHRS", 
            "ts": "1493135441.646772", 
            "attachments": [
                {
                    "thumb_height": 360, 
                    "service_icon": "https://a.slack-edge.com/2089/img/unfurl_icons/youtube.png", 
                    "thumb_width": 480, 
                    "author_link": "https://www.youtube.com/user/PyDataTV", 
                    "service_name": "YouTube", 
                    "title": "Gilles Louppe | Bayesian optimization with Scikit-Optimize", 
                    "video_html_height": 225, 
                    "author_name": "PyData", 
                    "title_link": "https://www.youtube.com/watch?v=DGJTEBt0d-s", 
                    "video_html": "<iframe width=\"400\" height=\"225\" src=\"https://www.youtube.com/embed/DGJTEBt0d-s?feature=oembed&autoplay=1&iv_load_policy=3\" frameborder=\"0\" allowfullscreen></iframe>", 
                    "video_html_width": 400, 
                    "service_url": "https://www.youtube.com/", 
                    "id": 1, 
                    "fallback": "YouTube Video: Gilles Louppe | Bayesian optimization with Scikit-Optimize", 
                    "thumb_url": "https://i.ytimg.com/vi/DGJTEBt0d-s/hqdefault.jpg", 
                    "from_url": "https://www.youtube.com/watch?v=DGJTEBt0d-s"
                }
            ]
        }, 
        {
            "text": "ugh i love sigopt. if only i had a phd", 
            "type": "message", 
            "user": "U21RUE8QZ", 
            "ts": "1492483719.502693"
        }, 
        {
            "text": "the latest (related to my lecture today and the adv stats course)\n<http://blog.sigopt.com/post/159685042073/sigopt-in-depth-expected-improvement-vs>", 
            "type": "message", 
            "user": "U1XU9SHRS", 
            "ts": "1492458915.289337", 
            "attachments": [
                {
                    "thumb_height": 128, 
                    "service_icon": "http://68.media.tumblr.com/avatar_7418dc40c1dc_128.png", 
                    "thumb_width": 128, 
                    "title": "SigOpt In Depth: Expected Improvement vs. Knowledge Gradient", 
                    "service_name": "SigOpt Blog", 
                    "title_link": "http://blog.sigopt.com/post/159685042073/sigopt-in-depth-expected-improvement-vs", 
                    "text": "By: Harvey Cheng. In previous blog posts, we discussed the intuition behind using Gaussian processes and compared the difference between using marginal maximum likelihood and kriging variance for estimating the hyperparameters of the covariance kernels. These insights contribute to building better surrogate models of the objective function to guide our search for an optimal solution. In this blog post, we discuss another ingredient of Bayesian optimization: the sampling policy (or acquisition function). The combination of these tools empowers SigOpt to help customers efficiently optimize their critical metrics, including the accuracy of machine learning models, the performance of a reinforcement learning algorithm and the output of complex simulations. Bayesian optimization in the Markov decision process framework Before we dive into the different types of sampling policies, we would like to familiarize the reader with the notations used in post. We want to look at the optimization loop in the context of a sequential decision making problem or, more precisely, a Markov decision process (MDP). In this setting, the decision maker alternates between choosing decisions ($x^n$) and observing outcomes from a stochastic process $\\{Y^n\\}$, as shown below: $$ \\text{choose } x^n \\rightarrow \\text{observe } Y^{n+1} \\rightarrow \\text{choose } x^{n+1} \\rightarrow \\text{observe } Y^{n+2} \\cdots $$ We define the state of the world ($S^n$) as all the relevant information sufficient to make a decision regarding $x^{n}$; in the standard definition, it is common to restrict the state to only include necessary information, so as to have minimal dimension. For this post, our state will always include the current estimate of the objective function, and certain sampling policies requiring additional information (such as the best incumbent observation). For Bayesian optimization, the decisions ($x^n$) are the suggestions (or the query points). For each decision, we receive a random/noisy observation ($Y_{x^n}^{n+1}$) from the customer(1); this is the exonogenous stochastic process. Using our knowledge of the state, the decision, and the observation, we can evolve to another state via a transition function $S^{n+1} = g( S^n, x^n, Y^{n+1} )$. Because of the random nature of the observations, the idea of the optimal decision does not apply since decisions are influenced by the random outcomes. Instead, the goal is to search for a policy that is optimal in expectation (or over some risk measure)(2). A policy is a decision rule consisting of a function mapping a state to a decision for each iteration $n$. A simple illustration Consider a problem where our decision space consists of five options, A through E, each with an outcome. Our goal is to discover the best option given a limited budget to sequentially sample the options. Let us further assume that observation from the samples are noisy. We have said that state variable encapsulates all the information required to compute the decision. If our state variable only contains the posterior mean of the options, as seen in Figure 1, where do we want to sample next if we want to discover the best option? Figure 1. The posterior means of the five options. In this simple example, we would want to sample option A since it has the highest estimated outcome. But what if we expand our state variable to include the posterior variance of the options, as shown in Figure 2? Figure 2. The full posterior distributions of the five options. With the added information, this becomes a more interesting decision. If your answer is still option A (because it has the highest estimated value), then you are exploiting. If you changed to option D (because it has the highest standard deviation), then you are exploring. A good sampling policy needs to carefully balance between exploitation and exploration. One class of policies involves defining an improvement criterion using the state variable for the next decision. The idea of using an improvement-based approach to help guide the sampling procedure can be traced back to Kushner, 1964, namely the probability of improvement policy. Expected improvement One of the most well known optimization criteria is the expected improvement (EI), first introduced in Mockus et al., 1978. This idea was combined with the Gaussian processes (GP) model in the efficient global optimization [Jones et al., 1998] and sequential kriging optimization [Huang et al., 2006]. These two algorithms form the foundation for most Bayesian optimization algorithms. Let $f^n(x)$ be the estimated mean of the objective function evaluate at $x$ after $n$ samples and $\\mathcal X$ be the decision space of the problem. The EI function is defined as $$EI^n(x) = \\textbf E \\Big[ \\max (f^{n+1} (x) \u2212 y_{\\text{best}}, 0 ) | S^n, x^n = x \\Big],$$ where the expectation is taken with respect to the random observation from decision $x^n$. The expected improvement policy is to select the maximal argument: $$X^{EI,n} = \\arg \\max_{x \\in \\mathcal X} EI^n(x).$$ The EI policy has become the de facto policy for Bayesian optimization because the EI function can be easily computed (with an analytical solution) under the Gaussian assumptions. Knowledge gradient A close variant of the EI algorithm is the knowledge gradient (KG) policy, which was introduced in Frazier et al., 2006 for finite discrete decision space with normally distributed alternatives. Scott et al., 2010 later extends the KG policy to work with GP models. The KG function is defined as $$ KG^n (x) = \\textbf E \\Big[ \\max_{x\u2019 \\in \\mathcal X} f^{n+1} (x\u2019) \u2212 \\max_{x\u2019 \\in \\mathcal X} f^n (x\u2019) | S^n, x^n = x \\Big], $$ where $\\max_{x \\in \\mathcal X} f(x)$ is the optimal value of the posterior mean. Analogously, the KG policy is then $$ X^{KG,n} = \\arg \\max_{x \\in \\mathcal X} KG^n(x). $$ The exact computation of the KG function is more costly than EI, due to the maximization within the expected value. A numerical comparison The EI and KG functions appear to be almost identical at a glance. In fact, these two are equivalent when the observations are deterministic(3). In the presence of noisy observations, these two policies behave very differently, but in what way? Let us revisit the example in Figure 2; we associate some numbers with the five options below in Table 1. posterior A B C D E mean 10 7 6 8.5 7 variance 0.5 1.0 0.5 2.5 2.5 Table 1. Posterior means and variances of the five options. Let us also assume the variance of the observation is 0.5 across all options and that the best incumbent outcome is 9.5. Computing the KG and EI values using these numbers, we can compare the results in Figure 3. Figure 3. The KG and EI values for the five options (in log scale) when our best incumbent outcome is 9.5. The KG policy prefers option D and the EI policy prefers option A. It is apparent from this table that the EI policy will select option A as the next sample; the KG policy, option D. The EI policy relies on the best incumbent solution as a guideline for the next sampling point, while the KG policy relies on the posterior distribution of the model for guideline. In a sense, the KG policy trusts the model and believes that option A has a lower marginal value (since it is already the optimal choice so far). The EI policy does not solely rely on the posterior distribution but also on the best incumbent solution. If our best incumbent result is 11.5 instead, the EI policy now suggests that we should be sampling option D next. Figure 4. The KG and EI values for the five options (in log scale) when our best incumbent outcome is 11.5. Both KG and EI policies prefers option D now. This difference in the policies also exists when we use a GP to model the objective function. In Figure 5, we can see that given the same GP model, EI policy wants to direct the next sample near the true optimal region of the function (also that of the posterior mean). On the other hand, KG policy wants to direct the sampling\u2026", 
                    "id": 1, 
                    "fallback": "SigOpt Blog: SigOpt In Depth: Expected Improvement vs. Knowledge Gradient", 
                    "thumb_url": "http://68.media.tumblr.com/avatar_7418dc40c1dc_128.png", 
                    "from_url": "http://blog.sigopt.com/post/159685042073/sigopt-in-depth-expected-improvement-vs"
                }
            ]
        }, 
        {
            "text": "<https://www.youtube.com/watch?v=_EYL_yUB0a4>", 
            "type": "message", 
            "user": "U1XU9SHRS", 
            "ts": "1490803536.518477", 
            "attachments": [
                {
                    "thumb_height": 360, 
                    "service_icon": "https://a.slack-edge.com/2089/img/unfurl_icons/youtube.png", 
                    "thumb_width": 480, 
                    "author_link": "https://www.youtube.com/channel/UCjeM1xxYb_37bZfyparLS3Q", 
                    "service_name": "YouTube", 
                    "title": "Alexandra Johnson, Software Engineer, SigOpt, NYC 2017", 
                    "video_html_height": 225, 
                    "author_name": "MLconf", 
                    "title_link": "https://www.youtube.com/watch?v=_EYL_yUB0a4", 
                    "video_html": "<iframe width=\"400\" height=\"225\" src=\"https://www.youtube.com/embed/_EYL_yUB0a4?feature=oembed&autoplay=1&iv_load_policy=3\" frameborder=\"0\" allowfullscreen></iframe>", 
                    "video_html_width": 400, 
                    "service_url": "https://www.youtube.com/", 
                    "id": 1, 
                    "fallback": "YouTube Video: Alexandra Johnson, Software Engineer, SigOpt, NYC 2017", 
                    "thumb_url": "https://i.ytimg.com/vi/_EYL_yUB0a4/hqdefault.jpg", 
                    "from_url": "https://www.youtube.com/watch?v=_EYL_yUB0a4"
                }
            ]
        }, 
        {
            "text": "^\"Common Problems in Hyperparameter Optimization\"", 
            "type": "message", 
            "user": "U1XU9SHRS", 
            "ts": "1490803474.483076"
        }, 
        {
            "text": "<https://www.slideshare.net/SessionsEvents/alexandra-johnson-software-engineer-sigopt>", 
            "type": "message", 
            "user": "U1XU9SHRS", 
            "ts": "1490803448.466979"
        }, 
        {
            "text": "GPs are kicking ass in that paper. SigOpt is paid and Spearmint is on a non-commercial research license", 
            "type": "message", 
            "user": "U21RUE8QZ", 
            "ts": "1490148950.301165"
        }, 
        {
            "text": "And: <https://arxiv.org/pdf/1603.09441.pdf>", 
            "type": "message", 
            "user": "U1XU9SHRS", 
            "ts": "1490145009.588905", 
            "edited": {
                "user": "U1XU9SHRS", 
                "ts": "1490145082.000000"
            }
        }, 
        {
            "text": "<http://blog.sigopt.com/post/141501625253/sigopt-for-ml-tensorflow-convnets-on-a-budget>", 
            "type": "message", 
            "user": "U1XU9SHRS", 
            "ts": "1490144954.578316", 
            "attachments": [
                {
                    "thumb_height": 128, 
                    "service_icon": "http://68.media.tumblr.com/avatar_7418dc40c1dc_128.png", 
                    "thumb_width": 128, 
                    "title": "SigOpt for ML: TensorFlow ConvNets on a Budget with Bayesian Optimization", 
                    "service_name": "SigOpt Blog", 
                    "title_link": "http://blog.sigopt.com/post/141501625253/sigopt-for-ml-tensorflow-convnets-on-a-budget", 
                    "text": "By: Ian Dewancker, Research Engineer In this post on integrating SigOpt with machine learning frameworks, we will show you how to use SigOpt and TensorFlow to efficiently search for an optimal configuration of a convolutional neural network (CNN). There are a large number of tunable parameters associated with defining and training deep neural networks ( Bergstra [1] ) and SigOpt accelerates searching through these settings to find optimal configurations. This search is typically a slow and expensive process, especially when using standard techniques like grid or random search, as evaluating each configuration can take multiple hours. SigOpt finds good combinations far more efficiently than these standard methods by employing an ensemble of state-of-the-art Bayesian optimization techniques, allowing users to arrive at the best models faster and cheaper. In this example, we consider the same optical character recognition task of the SVHN dataset as discussed in a previous post. Our goal is to build a model capable of recognizing digits (0-9) in small, real-world images of house numbers. We use SigOpt to efficiently find a good structure and training configuration for a convolutional neural net. Check out the code here if you\u2019d like to start experimenting! Convolutional Neural Net Structure The structure and topology of a deep neural network can have dramatic implications for performance on a given task ( Bengio [3] ). Many small decisions go into the connectivity and aggregation strategies for each of the layers that make up a deep neural net. These parameters can be non-intuitive to choose in an optimal, or even acceptable, fashion. In this experiment we used a TensorFlow CNN example designed for the MNIST dataset as a starting point. Figure 1 represents a typical CNN structure, highlighting the parameters we chose to vary in this experiment. A more complete discussion of these architectural decisions can be found in an online course from Stanford ( Li [5] ). It should be noted that Figure 1 is an approximation of the architecture used in this example, and the code serves as a more complete reference. Figure 1: Representative convolutional neural net topology. Important parameters include the width and depth of the convolutional filters, as well as dropout probability. (Sermanet [2]) TensorFlow has greatly simplified the effort required to build and experiment with deep neural network (DNN) designs. Tuning these networks, however, is still an incredibly important part of creating a successful model. The optimal structural parameters often highly depend on the dataset under consideration. SigOpt offers Bayesian optimization as a service to minimize the amount of trial and error required to find good structural parameters for DNNs and CNNs. Stochastic Gradient Descent Parameters ($\\alpha, \\beta, \\gamma$) Once the structure of the neural net has been selected, an optimization strategy based on stochastic gradient descent (SGD) is used to fit the weight parameters of the convolutional neural net. There is no shortage of SGD algorithm variations implemented in TensorFlow and several parametrizations of RMSProp, a particular SGD variation, are compared in Figure 2. Figure 2: Progression of RMSProp gradient descent with different parametrizations. left: Various decay rates with other parameters fixed: purple = .01, black = .5, red = .93. center: Various learning rates with other parameters fixed: purple = .016, black = .1, red = .6. right: Various momentums with other parameters fixed: purple = .2, black = .6, red = .93. It can be a counterintuitive and time consuming task to optimally configure a particular SGD algorithm for a given model and dataset. To simplify this tedious process, we expose to SigOpt the parameters that govern the RMSProp optimization algorithm. Important parameters governing its behavior are the learning rate ($\\alpha$) , momentum ($\\beta$) and decay ($\\gamma$) terms. These parameters define the RMSProp gradient update step: Algorithm 1: Pseudocode for RMSProp stochastic gradient descent. Stochastic gradient refers to the fact that we are estimating the loss function gradient using a subsample (batch) of the entire training data For this example, we used only a single epoch of the training data, where one epoch refers to a complete presentation of the entire training data (~500K images in our example). Batch size refers to the number of training examples used in the computation of each stochastic gradient (10K images in our example). One epoch is made up of several batch sized updates, so as to minimize the in-memory resources associated required for the optimization (Hinton [4]). Using only a single epoch can be detrimental to performance, but this was done in the interest of time for this example. Classification Performance To compare tuning the CNNs hyperparameters when using random search versus SigOpt, we ran 5 experiments using each method and compared the median best seen trace. The objective was the classification accuracy on a single 80 / 20 fold of the training and \u201cextra\u201d set of the SVHN dataset (71K + 500K images). The median best seen trace for each optimization strategy is shown below in Figure 3. In our experiment we allowed SigOpt and random search to perform 80 function evaluations (each representing a different proposed configuration of the CNN). A progression of the best seen objective at each evaluation for both methods is shown below in Figure 3. We include, as a baseline, the accuracy of an untuned TensorFlow CNN using the default parameters suggested in the official TensorFlow example. We also include the performance of a random forest classifier using sklearn defaults. Figure 3: Median best seen trace of CV accuracy over 5 independent optimization runs using SigOpt, random search as well as two baselines where no tuning was performed After hyperparameter optimization was completed for each method, we compared accuracy using a completely held out data set (SHVN test set, 26k images) using the best configuration found in the tuning phase. \u00a0The best hyperparameter configurations for each method in each of the 5 optimization runs was used for evaluation. The mean of these accuracies is reported in the table below. We also include the same baseline models described above and report their performance on the held out evaluation set. SigOpt (TensorFlow CNN) Random Search (TensorFlow CNN) No Tuning (sklearn RF) No Tuning (TensorFlow CNN) Hold out ACC 0.8130 ( +315.2% ) 0.5690 0.5278 0.1958 Table 1: Comparison of model accuracy on the held out dataset after different tuning strategies Cost Analysis Using SigOpt to optimize deep learning architectures instead of a standard approach like random search can translate to real savings in the total cost of tuning a model. This is especially true when expensive computational resources (for example GPU EC2 instances) are required by your modelling efforts. We compare the cost required to reach specific performances on the CV accuracy objective metric in our example experiment. Quickly finding optimal configurations has a direct savings on computational costs associated with tuning on top of the performance benefits of having a better model. Here we assume each observation costs $2.60, which is the cost per hour of using a single on-demand g2.8xlarge instance in EC2. Model Performance (CV Acc. threshold) Random Search Cost SigOpt Cost SigOpt Cost Savings Potential Production Savings (50 GPUs) 87% $275 $42 84% $12,530 85% $195 $23 88% $8,750 80% $46 $21 55% $1,340 70% $29 $21 27% $400 Table 2: Required costs for achieving same performance when tuning with SigOpt and random search. For CNNs in production more epochs are traditionally used; for this example we assume 50 GPUs and that the results scale perfectly with the parallelism. We observe that SigOpt offers a drastic discount in cost to achieve equivalent performance levels when compared with a standard method like rando\u2026", 
                    "id": 1, 
                    "fallback": "SigOpt Blog: SigOpt for ML: TensorFlow ConvNets on a Budget with Bayesian Optimization", 
                    "thumb_url": "http://68.media.tumblr.com/avatar_7418dc40c1dc_128.png", 
                    "from_url": "http://blog.sigopt.com/post/141501625253/sigopt-for-ml-tensorflow-convnets-on-a-budget"
                }
            ]
        }, 
        {
            "text": "<@U21RUE8QZ|chip> has joined the channel", 
            "ts": "1490144924.573038", 
            "subtype": "channel_join", 
            "user": "U21RUE8QZ", 
            "type": "message", 
            "inviter": "U1XU9SHRS"
        }, 
        {
            "text": "<@U233TELHZ|dasha> has joined the channel", 
            "ts": "1490144924.572996", 
            "subtype": "channel_join", 
            "user": "U233TELHZ", 
            "type": "message", 
            "inviter": "U1XU9SHRS"
        }, 
        {
            "text": "<@U1XU9SHRS|brian_spiering> has joined the channel", 
            "type": "message", 
            "user": "U1XU9SHRS", 
            "ts": "1490144922.572667", 
            "subtype": "channel_join"
        }
    ], 
    "channel_info": {
        "topic": {
            "last_set": 0, 
            "value": "", 
            "creator": ""
        }, 
        "is_general": false, 
        "name_normalized": "bayesian_opt_and_sa", 
        "name": "bayesian_opt_and_sa", 
        "is_channel": true, 
        "created": 1490144922, 
        "is_member": false, 
        "is_archived": false, 
        "creator": "U1XU9SHRS", 
        "is_org_shared": false, 
        "previous_names": [], 
        "purpose": {
            "last_set": 0, 
            "value": "", 
            "creator": ""
        }, 
        "members": [
            "U1XU9SHRS", 
            "U21RUE8QZ", 
            "U233TELHZ"
        ], 
        "id": "C4MMDP3AP", 
        "is_shared": false
    }
}